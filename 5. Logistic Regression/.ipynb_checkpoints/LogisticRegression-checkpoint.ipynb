{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning in Action.<br>\n",
    "Chapter 5 : Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important function for logistic regression:\n",
    "1. Sigmoid : 1 / (1 + e^-z). The return value represents the probability (该sample被分到类1的概率)\n",
    "2. Sigmoid input function z: z = wT x, wT is a vector, the weight coefficient vector\n",
    "3. Likehood function / cost function. (互为正负) Details about likehood function: https://www.zhihu.com/question/54082000/answer/470252492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def loadData(self, filename):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            load dataset\n",
    "        Args:\n",
    "            filename -- dataset filepath\n",
    "        Returns:\n",
    "            dataMat -- feature matrix\n",
    "            labelMat -- labels\n",
    "        \"\"\"\n",
    "        dataMat = []\n",
    "        labelMat = []\n",
    "        fr = open(filename)\n",
    "        for line in fr.readlines():\n",
    "            lineArr = line.strip().split() # split by blankspace\n",
    "            if len(lineArr) == 1:\n",
    "                continue\n",
    "            dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) # add x0 = 1 in each feature vector\n",
    "            labelMat.append(int(lineArr[2]))\n",
    "        return dataMat, labelMat\n",
    "    \n",
    "    def sigmoid(self, inX):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            calculate the sigmoid function result with giving feature vector\n",
    "        Args:\n",
    "            inX -- feature vector\n",
    "        Returns:\n",
    "            the sigmoid function result\n",
    "        \"\"\"\n",
    "        # return 1.0 / (1 + exp(-inX))\n",
    "        # The function Tanh's mean is 0, so the performance of tanh will be better than sigmoid\n",
    "        return 1.0 / (1 + np.exp(-inX))\n",
    "    \n",
    "    def plotOriData(self, dataArr, labelMat):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            visualize the original data (scatter plot)\n",
    "        Args:\n",
    "            dataArr -- feature matrix\n",
    "            labelMat -- labels\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        dataArr = np.array(dataArr)\n",
    "        n = np.shape(dataArr)[0] # sample number\n",
    "        xcord1 = []\n",
    "        ycord1 = []\n",
    "        xcord2 = []\n",
    "        ycord2 = []\n",
    "        for i in range(n):\n",
    "            # split samples according to their label value\n",
    "            # group the sample with label = 1 (category 1)\n",
    "            if labelMat[i] == 1: \n",
    "                xcord1.append(dataArr[i, 1]) # the first feature, it should be noted that x0 = 1\n",
    "                ycord1.append(dataArr[i, 2]) # the second feature\n",
    "            \n",
    "            # group the sample with label = 0 (category 2)\n",
    "            else:\n",
    "                xcord2.append(dataArr[i, 1]) # the first feature, it should be noted that x0 = 1\n",
    "                ycord2.append(dataArr[i, 2]) # the second feature\n",
    "                \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') # use red point to represent the samples in category 1\n",
    "        ax.scatter(xcord2, ycord2, s=30, c='green') # use green point to represent the samples in category 2\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.show()\n",
    "    \n",
    "    def gradAscent(self, dataMatIn, classLabels):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            gradient Ascient is used to find the max value of log-likehood function. \n",
    "            Details can be found in https://blog.csdn.net/achuo/article/details/51160101\n",
    "            gradient descent is used to find the min value of cost function.\n",
    "        Args:\n",
    "            dataMatIn -- feature matrix\n",
    "            classLabels -- sample labels\n",
    "        Returns:\n",
    "            weight: the weight coefficient vector\n",
    "        \n",
    "        \"\"\"\n",
    "        dataMatrix = np.mat(dataMatIn)\n",
    "        labelMat = np.mat(classLabels).transpose() # convert list type into np.mat type, then convert row vector into column vector\n",
    "        m, n = np.shape(dataMatrix) # m is sample number, n is feature number\n",
    "        alpha = 0.001 # learning rate\n",
    "        maxLoop = 500\n",
    "        weights = np.ones((n, 1)) # initial weight vector\n",
    "        for k in range(maxLoop):\n",
    "            h = self.sigmoid(dataMatrix * weights) # (m,n)的矩阵dataMatrix乘以(n,1)的向量weights,得到h的结果(m,1)的向量\n",
    "            error = labelMat - h # that's why labelMat also need to be converted into (m, 1) column vector\n",
    "            # h and error are (m, 1), xij is (m, n), so the gradient update should be xij.T * h, so that we can get a (n, 1)\n",
    "            weights = weights + alpha * dataMatrix.transpose() * error\n",
    "        return np.array(weights)\n",
    "    \n",
    "    def storGradAscent0(self, dataMatrix, classLabels):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            random gradient ascent. 增量更新, only use one sample to update weight coeffients in one step.\n",
    "        Args:\n",
    "            dataMatrix -- feature matrix\n",
    "            classLabels -- sample labels\n",
    "        Returns:\n",
    "            weights -- weight coeffient vector\n",
    "        \"\"\"\n",
    "        m, n = np.shape(dataMatrix)\n",
    "        weights = np.ones((n))\n",
    "        alpha = 0.001\n",
    "        maxLoop = 500\n",
    "        for i in range(m):\n",
    "            # weights is a vector with n as length, weights is a vector with n as length\n",
    "            # In Python, the result of multiplication between two vector:\n",
    "            # >>> np.array([1, 2, 3]) * np.array([1, 2, 3])\n",
    "            # >>> array([1, 4, 9])\n",
    "            # so we use sum to calculate h\n",
    "            # if we use np.mat, the multiplication between two np.mat should obey the law of matrix multiplication\n",
    "            h = self.sigmoid(sum(dataMatrix[i] * weights)) # use sum to convert mat into a value, h is a value, not vector\n",
    "            error = labelMat[i] - h\n",
    "            weights = weights + alpha * dataMatrix[i] * error \n",
    "        return weights\n",
    "\n",
    "    def storGradAscent1(self, dataMatrix, classLabels, numIter = 150):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            random gradient ascent. 增量更新, only use one sample to update weight coeffients in one step.\n",
    "            In each step, randomly choose one sample and use it to update weight vector, then delete it.\n",
    "            This guaranteed that no sample will be chosen again.\n",
    "        Args:\n",
    "            dataMatrix -- feature matrix\n",
    "            classLabels -- sample labels\n",
    "        Returns:\n",
    "            weights -- weight coeffient vector\n",
    "        Remarks:\n",
    "            可以借鉴的点：\n",
    "            1. 随迭代次数逐渐减小learning rate\n",
    "            2. 增量式学习方法。 每次随机选取一个sample更新weights，然后删除该sample， 再随机选择下一个sample\n",
    "            3. 无放回的随机抽样算法可以参考该function的写法\n",
    "        \"\"\"\n",
    "        m, n = np.shape(dataMatrix)\n",
    "        weights = np.ones((n))\n",
    "        for j in range(numIter):\n",
    "            dataIndex = list(range(m))\n",
    "            for i in range(m):\n",
    "                alpha = 4 / (1.0 + j + i) + 0.01 # convert alpha from constant into a variable\n",
    "                # np.random.uniform(x, y): generate a random value, min is x, max is y\n",
    "                randIndex = int(np.random.uniform(0, len(dataIndex))) # generate a random index used to update weights\n",
    "                h = self.sigmoid(np.sum(dataMatrix[dataIndex[randIndex]] * weights)) # use sum to convert mat into a value, h is a value, not vector\n",
    "                error = labelMat[dataIndex[randIndex]] - h\n",
    "                weights = weights + alpha * dataMatrix[dataIndex[randIndex]] * error \n",
    "                del(dataIndex[randIndex])\n",
    "        return weights\n",
    "    \n",
    "    def stoc_grad_ascent11(self, data_mat, class_labels, num_iter=150):\n",
    "        \"\"\"\n",
    "        改进版的随机梯度上升，使用随机的一个样本来更新回归系数\n",
    "        :param data_mat: 输入数据的数据特征（除去最后一列）,ndarray\n",
    "        :param class_labels: 输入数据的类别标签（最后一列数据\n",
    "        :param num_iter: 迭代次数\n",
    "        :return: 得到的最佳回归系数\n",
    "        \"\"\"\n",
    "        m, n = np.shape(data_mat)\n",
    "        weights = np.ones(n)\n",
    "        for j in range(num_iter):\n",
    "            # 这里必须要用list，不然后面的del没法使用\n",
    "            data_index = list(range(m))\n",
    "            for i in range(m):\n",
    "                # i和j的不断增大，导致alpha的值不断减少，但是不为0\n",
    "                alpha = 4 / (1.0 + j + i) + 0.01\n",
    "                # 随机产生一个 0～len()之间的一个值\n",
    "                # random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。\n",
    "                rand_index = int(np.random.uniform(0, len(data_index)))\n",
    "                h = self.sigmoid(np.sum(data_mat[data_index[rand_index]] * weights))\n",
    "                error = class_labels[data_index[rand_index]] - h\n",
    "                weights = weights + alpha * error * data_mat[data_index[rand_index]]\n",
    "                del(data_index[rand_index])\n",
    "        return weights\n",
    "\n",
    "        \n",
    "    def plotBestFit(self, dataArr, labelMat, weights):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            visualize the original data (scatter plot)\n",
    "        Args:\n",
    "            dataArr -- feature matrix\n",
    "            labelMat -- labels\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        dataArr = np.array(dataArr)\n",
    "        n = np.shape(dataArr)[0] # sample number\n",
    "        xcord1 = []\n",
    "        ycord1 = []\n",
    "        xcord2 = []\n",
    "        ycord2 = []\n",
    "        for i in range(n):\n",
    "            # split samples according to their label value\n",
    "            # group the sample with label = 1 (category 1)\n",
    "            if labelMat[i] == 1: \n",
    "                xcord1.append(dataArr[i, 1]) # the first feature, it should be noted that x0 = 1\n",
    "                ycord1.append(dataArr[i, 2]) # the second feature\n",
    "            \n",
    "            # group the sample with label = 0 (category 2)\n",
    "            else:\n",
    "                xcord2.append(dataArr[i, 1]) # the first feature, it should be noted that x0 = 1\n",
    "                ycord2.append(dataArr[i, 2]) # the second feature\n",
    "                \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') # use red point to represent the samples in category 1\n",
    "        ax.scatter(xcord2, ycord2, s=30, c='green') # use green point to represent the samples in category 2\n",
    "        # x为x1（第一个特征值），y为x2（第二个特征值）,我们要画的拟合线为w0*x0 + w1*x1 + w2*x2 = 0\n",
    "        x = np.arange(-3.0, 3.0, 0.1)\n",
    "        y = (-weights[0] - weights[1] * x) / weights[2]\n",
    "        ax.plot(x, y)        \n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient Ascent update:\n",
    "<img src=\"gradAscent.jfif\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "dataMat, labelMat = LR.loadData(r'..\\data\\Ch05\\testSet.txt')\n",
    "#LR.plotOriData(dataMat, labelMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataArr = np.array(dataMat)\n",
    "weights = LR.storGradAscent1(dataArr, labelMat)\n",
    "LR.plotBestFit(dataArr, labelMat, weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
