{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">\n",
    "Random Forest implenmentation<br>\n",
    "@Author: Ge Chen<br>\n",
    "@Time: 2019-8-21<br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter introduced some ensemble methods.<br>\n",
    "The ensemble methods can be divided into two types:\n",
    "    1. bagging (Random Forest), 是基于数据随机重抽样分类器构造的方法\n",
    "    2. boosting (AdaBoost), 是基于所有分类器的加权求和的方法\n",
    "The difference between bagging and boosting:\n",
    "1. bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。\n",
    "2. bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。\n",
    "3. bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import seed, randrange, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def loadDataSet(self, filename):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            load dataset from .txt file\n",
    "        Args:\n",
    "            filename -- .txt file path\n",
    "        Returns:\n",
    "            dataSet -- data set, contain feature matrix and labels, list type\n",
    "        \"\"\"\n",
    "        dataSet = []\n",
    "        with open(filename) as fr:\n",
    "            for line in fr.readlines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                lineArr = []\n",
    "                # It should be noted that the element splitted\n",
    "                # from line is string type\n",
    "                for feature in line.split(\",\"):\n",
    "                    str_f = feature.strip()\n",
    "                    try:\n",
    "                        lineArr.append(float(str_f))\n",
    "                    except:\n",
    "                        lineArr.append(str_f)\n",
    "                dataSet.append(lineArr)\n",
    "        return dataSet\n",
    "\n",
    "    def cross_validation_split(self, dataset, n_folds):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            make n_fold dataset from original dataset. (有放回抽样，用于CV)\n",
    "        Args:\n",
    "            dataset -- original dataset\n",
    "            n_folds -- the dataset number we want to make\n",
    "        Returns:\n",
    "            dataset_split -- list type, one element is a dataset fold.\n",
    "                             list集合，存放的是：将数据集进行抽重抽样 n_folds 份，\n",
    "                                 数据可以重复重复抽取，每一次list的元素是无重复的\n",
    "        \"\"\"\n",
    "        dataset_split = []\n",
    "        dataset_copy = dataset.copy()\n",
    "        fold_size = (len(dataset) / n_folds)\n",
    "        for i in range(n_folds):\n",
    "            fold = []\n",
    "            while len(fold) < fold_size:\n",
    "                # generate random integer as the selected index\n",
    "                index = randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy[index]) # 有放回抽样\n",
    "                fold.append(dataset_copy.pop(index)) # 无放回抽样\n",
    "            dataset_split.append(fold)\n",
    "        return dataset_split\n",
    "    \n",
    "    def subsample(self, dataset, ratio):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            create the subdataset with random sampling.\n",
    "        Args:\n",
    "            dataset -- training dataset\n",
    "            ratio -- the ratio of subdataset in training dataset\n",
    "        Retures:\n",
    "            samples -- the sample collection with random sampling\n",
    "        \"\"\"\n",
    "        sample = []\n",
    "        # calculate the length of subdataset\n",
    "        # round(): 四舍五入\n",
    "        n_sample = round(len(dataset) * ratio)\n",
    "        while len(sample) < n_sample:\n",
    "            # random generate integer value as sample index\n",
    "            # 有放回采样，one sample can occur in a subdataset for multiple times\n",
    "            index = randrange(len(dataset))\n",
    "            sample.append(dataset[index])\n",
    "        return sample\n",
    "    \n",
    "    def test_split(self, index, value, dataset):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            split dataset with giving feature value\n",
    "        Args:\n",
    "            index -- feature index\n",
    "            value -- feature value (delimiting standard)\n",
    "            dataset -- dataset waiting to be splitted\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        left, right = [], []\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "    \n",
    "    def gini_index(self, groups, class_values):\n",
    "        \"\"\"\n",
    "        Gini指数的计算问题，假如将原始数据集D切割两部分，分别为D1和D2，则\n",
    "        Gini(D|切割) = (|D1|/|D| ) * Gini(D1) + (|D2|/|D|) * Gini(D2)\n",
    "        \n",
    "        Desc:\n",
    "            calculate the gini index with giving groups and class values\n",
    "        Args:\n",
    "            groups -- groups after splitting\n",
    "            class_values -- class value used to calculate gini index\n",
    "        Returns:\n",
    "            gini -- gini index\n",
    "        \"\"\"\n",
    "        gini = 0.0\n",
    "        D = sum(len(group) for group in groups)\n",
    "        for class_value in class_values:\n",
    "            for group in groups:\n",
    "                size = len(group)\n",
    "                if size == 0: # 说明这个特征下，所有值相同，因此其中一个group size为0\n",
    "                    continue # size=0这种情况下gini = 0\n",
    "                # calculate the proportion for each class_value in every group\n",
    "                proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "                gini += float(size) / D * proportion * (1 - proportion)\n",
    "        return gini\n",
    "    \n",
    "    def get_split(self, dataset, n_features):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            1. random select n features for sub training data \n",
    "                and generate the sub training set.\n",
    "            2. In each training subdataset, traverse each feature and \n",
    "                find the best feature (minimize gini index)\n",
    "            3. obtain the best feature index, the best value to split b_feature\n",
    "                and the groups after splitting.\n",
    "        Args:\n",
    "            dataset -- dataset, usually the subdataset (a fold) in this RF code\n",
    "            n_features -- random selected feature number\n",
    "        Returns:\n",
    "            b_index -- best feature index\n",
    "            value -- the value used to split best feature\n",
    "            b_groups -- groups after splitting.\n",
    "        \"\"\"\n",
    "        # get the unique value of labels\n",
    "        class_values = list(set(row[-1] for row in dataset))\n",
    "        # initialize the best value index, value, score and groups\n",
    "        b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "        \n",
    "        # random select features\n",
    "        features = [] # initialization\n",
    "        while len(features) < n_features:\n",
    "            index = randrange(len(dataset[0]) - 1) # feature index\n",
    "            if index not in features:\n",
    "                features.append(index)\n",
    "        \n",
    "        # find the best feature which minimize gini index\n",
    "        for index in features:\n",
    "            for row in dataset:\n",
    "                # split subdataset with giving feature index and feature value\n",
    "                # the subgroup will be divided into two parts\n",
    "                # 1. samples[index] < giving value, 2. the rest\n",
    "                # 使用给定值划分数据集，划分后，第一部分包含所有该特征值\n",
    "                #  小于给定值的sample； 第二部分包含剩下的所有sample\n",
    "                groups = self.test_split(index, row[index], dataset)\n",
    "                # calculate gini index with giving splitting method and class_Value\n",
    "                gini = self.gini_index(groups, class_values)\n",
    "                \n",
    "                # find the minimum gini\n",
    "                if gini < b_score:\n",
    "                    b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "        return {'index':b_index, 'value':b_value, 'groups': b_groups}\n",
    "    \n",
    "    def to_terminal(self, group):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            find the most frequent labels in this group\n",
    "        Args:\n",
    "            group -- sample set in this node\n",
    "        Returns:\n",
    "            the most frequent labels\n",
    "        \"\"\"\n",
    "        outcomes = [row[-1] for row in group]\n",
    "        return max(set(outcomes), key=outcomes.count)\n",
    "    \n",
    "    def split(self, node, max_depth, min_size, n_features, depth):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            split the node and generate the leaf node\n",
    "        Args:\n",
    "            node -- parent node, contain {'index':b_index, 'value':b_value, 'groups': b_groups}\n",
    "            max_depth -- maximum depth for decision tree (最大递归次数)\n",
    "            min_size -- minimum sample size for leaf node\n",
    "            n_features -- selected feature number\n",
    "            depth -- the depth from root node to this node (记录此时的递归次数) \n",
    "        \"\"\"\n",
    "        left, right = node['groups'] # get the dataset in left leaf node and right leaf node\n",
    "        \n",
    "        # determine the recursion ending conditions\n",
    "        # Ending condition 1: if the feature value in a leaf node is the same\n",
    "        # In other words, the  left or right is empty\n",
    "        if not left or not right:\n",
    "            # 这种情况说明此node上的samples无法再根据featutre value划分来一进步减小gini_index\n",
    "            # 说明此Node上的samples已经划分完毕（无法再划分生成子节点）\n",
    "            # 因此直接返回出现次数较多的label\n",
    "            node['left'] = node['right'] = self.to_terminal(left + right)\n",
    "            return\n",
    "        \n",
    "        # Ending condition 2: depth >= max_depth\n",
    "        if depth >= max_depth:\n",
    "            node['left'], node['right'] = self.to_terminal(left), self.to_terminal(right)\n",
    "            return\n",
    "        \n",
    "        # Ending condition3: sample size of leaf node <= min_size\n",
    "        if len(left) <= min_size:\n",
    "            node['left'] = self.to_terminal(left)\n",
    "        else:\n",
    "            # 运行到这里，说明该Node还可以继续划分，使用递归进行下一次的划分\n",
    "            node['left'] = self.get_split(left, n_features)\n",
    "            self.split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "            \n",
    "        if len(right) <= min_size:\n",
    "            node['right'] = self.to_terminal(right)\n",
    "        else:\n",
    "            # 运行到这里，说明该Node还可以继续划分，使用递归进行下一次的划分\n",
    "            node['right'] = self.get_split(right, n_features)\n",
    "            self.split(node['right'], max_depth, min_size, n_features, depth+1)       \n",
    "        \n",
    "    \n",
    "    \n",
    "    def build_tree(self, train, max_depth, min_size, n_features):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            build a decision tree with giving training set and feature number.\n",
    "        Args:\n",
    "            train -- training set\n",
    "            max_depth -- the maximum depth for each decision tree\n",
    "            min_size -- the minimum size for the leaf node\n",
    "            n_features -- the selected feature number\n",
    "        Returns:\n",
    "            root -- decision tree\n",
    "        \"\"\"\n",
    "        root = self.get_split(train, n_features) # get the best feature and corresponding value\n",
    "        # 这里的root是一个root node, 将采用递归的方式不断建立子Node，并保存在node[left],node[right]中\n",
    "        # 由于每个特征都是二分类的（大于或小于等于给定值），因此建立的所有decision tree为二叉树\n",
    "        self.split(root, max_depth, min_size, n_features, 1)\n",
    "        # 递归完成后，形成完整的decision tree，\n",
    "        # 每个节点包含{'index':b_index, 'value':b_value, 'groups': b_groups}\n",
    "        return root\n",
    "    \n",
    "    def predict(self, node, sample):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            classify the sample \n",
    "        Args:\n",
    "            node -- node in a decision tree\n",
    "            sample -- sample waiting to be classified\n",
    "        Returns:\n",
    "            the predict label for the giving sample\n",
    "        \"\"\"\n",
    "        # judge whether this sample belongs to left group or not\n",
    "        if sample[node['index']] < node['value']: # in this case, this sample belongs to left group\n",
    "            # if there is a leaf node on the left, use recursion to classify this sample\n",
    "            if isinstance(node['left'], dict): \n",
    "                return self.predict(node['left'], sample)\n",
    "            # if there is no leaf node on the left, return the labels of \n",
    "            # left group (most frequent labels)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else: # in this case, the sample belongs to right group\n",
    "            # the classification method is the same with that in left group\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.predict(node['right'], sample)\n",
    "            else:\n",
    "                return node['right']\n",
    "    \n",
    "    def bagging_predict(self, trees, sample):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            bagging prediction.\n",
    "        Args:\n",
    "            trees -- the forest formed by decision trees\n",
    "            sample -- the sample waiting to be classified\n",
    "        Returns:\n",
    "            the most frequent classified result\n",
    "        \"\"\"\n",
    "        predictions = [self.predict(tree, sample) for tree in trees]\n",
    "        return max(set(predictions), key = predictions.count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def random_forest(self, train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            1. train a random forest model with training set\n",
    "            2. predict labels for test set\n",
    "        Args:\n",
    "            train -- training set\n",
    "            test -- test set\n",
    "            max_depth -- maximum depth for decision tree\n",
    "            min_size -- minimum sample size for leaf node\n",
    "            sample_size -- the ratio of randomly sampling set in training set\n",
    "            n_trees -- the decision tree number\n",
    "            n_features -- selected feature number\n",
    "        Returns:\n",
    "            predictions -- the predicted labels for test set by using random forest model\n",
    "        \"\"\"\n",
    "        trees = []\n",
    "        for i in range(n_trees):\n",
    "            # randomly select samples from training set to keep the difference of decision trees.\n",
    "            sample = self.subsample(train, sample_size)\n",
    "            # build a decision tree\n",
    "            tree = self.build_tree(sample, max_depth, min_size, n_features)\n",
    "            # generate a decision tree collection to form the random forest\n",
    "            trees.append(tree)\n",
    "        \n",
    "        # predict the labels for test set with the trained random forest model\n",
    "        predictions = [self.bagging_predict(trees, row) for row in test]\n",
    "        return predictions\n",
    "    \n",
    "    def accuracy_metric(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            calculate the accuracy for the test set\n",
    "        Args:\n",
    "            actual -- the actual labels of test set\n",
    "            predicted -- the predicted labels of test set\n",
    "        Returns:\n",
    "            the accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == predicted[i]:\n",
    "                correct += 1\n",
    "        return correct / float(len(actual)) * 100.0\n",
    "        \n",
    "    def evaluate_algorithm(self, dataset, algorithm, n_folds, *args):\n",
    "        \"\"\"\n",
    "        Desc:\n",
    "            evaluate the algorithm performance and get the scores\n",
    "        Args:\n",
    "            dataset -- original dataset\n",
    "            algorithm -- the algorithm used to classify samples\n",
    "            n_folds -- fold number in CV\n",
    "            *args -- other parameters\n",
    "        Returns:\n",
    "            scores -- model performance scores\n",
    "        \"\"\"\n",
    "        # split dataset into several folds to realize cross validation\n",
    "        folds = self.cross_validation_split(dataset, n_folds)\n",
    "        scores = []\n",
    "        for fold in folds:\n",
    "            train_set = list(folds)\n",
    "            train_set.remove(fold)\n",
    "            #In [25]: l\n",
    "            #Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]\n",
    "            #In [26]: sum(l, [])\n",
    "            #Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]\n",
    "            train_set = sum(train_set, []) # generate training set\n",
    "            test_set = []\n",
    "            for row in fold:\n",
    "                row_copy = list(row) # make a copy from every sample\n",
    "                row_copy[-1] = None\n",
    "                test_set.append(row_copy) # create test set for CV (delete the labels)\n",
    "            # In each CV step, train a random forest model and predict labels for test set\n",
    "            # Then, calculate the accuracy as score for each CV step\n",
    "            predicted = self.random_forest(train_set, test_set, *args)\n",
    "            actual = [row[-1] for row in fold]  \n",
    "            accuracy = self.accuracy_metric(actual, predicted)\n",
    "            scores.append(accuracy)\n",
    "        return scores\n",
    "                \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random= 0.13436424411240122\n",
      "Trees: 1\n",
      "Scores: [42.857142857142854, 85.71428571428571, 85.71428571428571, 66.66666666666666, 76.19047619047619]\n",
      "Mean Accuracy: 71.429%\n",
      "random= 0.13436424411240122\n",
      "Trees: 10\n",
      "Scores: [71.42857142857143, 76.19047619047619, 85.71428571428571, 71.42857142857143, 76.19047619047619]\n",
      "Mean Accuracy: 76.190%\n",
      "random= 0.13436424411240122\n",
      "Trees: 20\n",
      "Scores: [71.42857142857143, 85.71428571428571, 90.47619047619048, 71.42857142857143, 71.42857142857143]\n",
      "Mean Accuracy: 78.095%\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForest()\n",
    "dataset = RF.loadDataSet(r'..\\data\\Ch07\\sonar-all-data.txt')\n",
    "n_folds = 5\n",
    "max_depth = 20\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = 15\n",
    "for n_trees in [1, 10, 20]:\n",
    "    scores = RF.evaluate_algorithm(dataset, RF.random_forest, n_folds, max_depth,\n",
    "                                   min_size, sample_size, n_trees, n_features)\n",
    "    seed(1)\n",
    "    print('random=', random())\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
