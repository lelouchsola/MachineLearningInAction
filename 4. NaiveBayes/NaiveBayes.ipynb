{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def load_data_set(self):\n",
    "        # fake dataset created by ourself\n",
    "        \"\"\"\n",
    "        posting_list: feature dataset\n",
    "        class_vec: labels\n",
    "        \"\"\"\n",
    "        posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "        class_vec = [0, 1, 0, 1, 0, 1]  # 1 is 侮辱性的文字, 0 is not\n",
    "        return posting_list, class_vec\n",
    "    \n",
    "    def createVocabList(self, dataSet):\n",
    "        \"\"\"\n",
    "        return the unique vocabulary list\n",
    "        \"\"\"\n",
    "        vocabSet = set([])\n",
    "        for document in dataSet:\n",
    "            vocabSet = vocabSet | set(document)\n",
    "        return list(vocabSet)\n",
    "    \n",
    "    def setOfWords2Vec(self, vocabList, inputSet):\n",
    "        \"\"\"\n",
    "        convert document into vector (if a vocabulary occurred, then the corresponding\n",
    "        feature will be set as 1)\n",
    "        \"\"\"\n",
    "        returnVec = [0] * len(vocabList) # the length of feature vector = the length of vocabList\n",
    "        for word in inputSet:\n",
    "            if word in vocabList:\n",
    "                returnVec[vocabList.index(word)] = 1\n",
    "            else:\n",
    "                print (\"the word: %s is not in my VocabList\" % word)\n",
    "        return returnVec\n",
    "    \n",
    "    def trainNB0(self, trainMatrix, trainCategory):\n",
    "        \"\"\"\n",
    "        input:\n",
    "        1. trainMatrix: training documents\n",
    "        2. trainCategory: corresponding training labels\n",
    "        return:\n",
    "        1. p0Vect: vector, each element is p(wi|C0).\n",
    "        2. p1Vect: vector, each element is p(wi|C1).\n",
    "        3. pAbusive: p(C1)\n",
    "        \n",
    "        Note: p(wi|C0) represents the probability that the word (VocalList[i]) \n",
    "                occur in all C0 documents\n",
    "        \"\"\"\n",
    "        numTrainDocs = len(trainMatrix) # row number, the trianing file number\n",
    "        numWords = len(trainMatrix[0]) # column number, vocabList length\n",
    "        \"\"\"calculate the probability of Abusive file, p(ci)\"\"\"\n",
    "        pAbusive = sum(trainCategory) / float(numTrainDocs) # the probability of Abusive file\n",
    "        \n",
    "        # generate the empty vector to store the probability for each word in every category\n",
    "#         p0Num = np.zero(numWords)\n",
    "#         p1Num = np.zero(numWords)\n",
    "        \"\"\"we set the initial frequence as 1 for every word to avoid 0\"\"\"\n",
    "        p0Num = np.ones(numWords)\n",
    "        p1Num = np.ones(numWords)\n",
    "        \n",
    "        # create variable to store the total word number in each category\n",
    "#         p0Denom = 0.0\n",
    "#         p1Denom = 0.0\n",
    "        \"\"\"we also set the initial total number as 2 to avoid 0\"\"\"\n",
    "        p0Denom = 2.0\n",
    "        p1Denom = 2.0\n",
    "        \n",
    "        # count the frequence of each word and the total word number\n",
    "        for i in range(numTrainDocs):\n",
    "            if trainCategory[i] == 1: # this document belongs to category 1\n",
    "                p1Num += trainMatrix[i] # cumsum the trainMatrix vector\n",
    "                p1Denom += sum(trainMatrix[i]) # cumsum the total word number of each document\n",
    "            else:\n",
    "                p0Num += trainMatrix[i]\n",
    "                p0Denom += sum(trainMatrix[i])\n",
    "        \n",
    "        # calculate p(wi|C0) and p(wi|C1)\n",
    "#         p0Vect = p0Num / p0Denom\n",
    "#         p1Vect = p1Num / p1Denom\n",
    "        \n",
    "        \"\"\"\n",
    "        另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 \n",
    "        p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非\n",
    "        常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多\n",
    "        很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。\n",
    "        在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或\n",
    "        者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate In(p(wi|C0)) and In(p(wi|C1))\n",
    "        p0Vect = np.log(p0Num / p0Denom) #numpy.log: Natural logarithm, element-wise.\n",
    "        p1Vect = np.log(p1Num / p1Denom)\n",
    "        \n",
    "        return p0Vect, p1Vect, pAbusive\n",
    "    \n",
    "    def classifyNB(self, vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "        \"\"\"\n",
    "        function: classify the input document\n",
    "        input:\n",
    "        1. vec2Classify: the document waiting to be classified\n",
    "        2. p0Vec: In(p(wi|C0)) vector\n",
    "        3. p1Vec: In(p(wi|C1)) vector\n",
    "        4. pClass1: p(C1)\n",
    "        return:\n",
    "        classified result, 1 or 0\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate p(C1|w) and p(C0|w), w is a vector\n",
    "        # vector multiplication, element i (vector 0) * element i (vector 1)\n",
    "        p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "        p0 = sum(vec2Classify * p0Vec) + np.log(1 - pClass1)\n",
    "        \n",
    "        if p1 > p0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def testNB(self):\n",
    "        listOPosts, listClasses = self.load_data_set()\n",
    "        myVocablist = self.createVocabList(listOPosts)\n",
    "        trainMat = []\n",
    "\n",
    "        # create the training matrix \n",
    "        for postinDoc in listOPosts:\n",
    "            trainMat.append(self.setOfWords2Vec(myVocablist, postinDoc))\n",
    "\n",
    "        # calculate the probability vector and category probability\n",
    "        p0v, p1v, pAb = NB.trainNB0(trainMat, listClasses)\n",
    "\n",
    "        # test the model, classify the test document\n",
    "        testEntry = ['love', 'my', 'dalmation']\n",
    "        thisDoc = np.array(self.setOfWords2Vec(myVocablist, testEntry))\n",
    "        print('the result is: {}'.format(self.classifyNB(thisDoc, p0v, p1v, pAb)))\n",
    "\n",
    "        testEntry = ['stupid', 'garbage']\n",
    "        thisDoc = np.array(NB.setOfWords2Vec(myVocablist, testEntry))\n",
    "        print('the result is: {}'.format(self.classifyNB(thisDoc, p0v, p1v, pAb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is: 0\n",
      "the word: garbage is not in my VocabList\n",
      "the result is: 1\n"
     ]
    }
   ],
   "source": [
    "NB = NaiveBayes()\n",
    "NB.testNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spamTestClassifier():\n",
    "    def __init__(self):\n",
    "        self.reEx = r'\\W+' # define the regular expression used in spliting string\n",
    "        \n",
    "    def textParse(self, bigString):\n",
    "        listOfTokens = re.split(self.reEx, bigString)\n",
    "        # remain the token which length exceeds 2.\n",
    "        return [tok.lower() for tok in listOfTokens if len(tok)>2]\n",
    "    \n",
    "    def spam_test(self):\n",
    "        doc_list = [] # store the vocabulary list for each text\n",
    "        full_text = [] # flip all text and store all vocabulary in every text\n",
    "        class_list = [] # store the label\n",
    "        for i in range(1, 26):\n",
    "            # split the spam text and get the vocabulary set \n",
    "            try:\n",
    "                words = self.textParse(open('..\\data\\Ch04\\email\\spam\\{}.txt'.format(i)).read())\n",
    "            except:\n",
    "                words = self.textParse(open('..\\data\\Ch04\\email\\spam\\{}.txt'.format(i), \n",
    "                                            encoding='Windows 1252').read())\n",
    "            doc_list.append(words)\n",
    "            full_text.extend(words)\n",
    "            class_list.append(1)\n",
    "            \n",
    "            # split the ham text and get the vocabulary set \n",
    "            try:\n",
    "                words = self.textParse(open('..\\data\\Ch04\\email\\ham\\{}.txt'.format(i)).read())\n",
    "            except:\n",
    "                words = self.textParse(open('..\\data\\Ch04\\email\\ham\\{}.txt'.format(i), \n",
    "                                            encoding='Windows 1252').read())\n",
    "            doc_list.append(words)\n",
    "            full_text.extend(words)\n",
    "            class_list.append(0)\n",
    "        \n",
    "        # generate unique vocabulary list\n",
    "        NB = NaiveBayes()\n",
    "        vocabList = NB.createVocabList(doc_list)\n",
    "        \n",
    "        # generate 10 integer as the test set index\n",
    "        testSetIndex = [int(num) for num in random.sample(range(50), 10)]\n",
    "        trainingSetIndex = list(set(range(50)) - set(testSetIndex)) # get the training set index\n",
    "        \n",
    "        # convert the text in training set into feature vector\n",
    "        trainingMat = []\n",
    "        trainingClass = []\n",
    "        for doc_index in trainingSetIndex:\n",
    "            trainingMat.append(NB.setOfWords2Vec(vocabList, doc_list[doc_index]))\n",
    "            trainingClass.append(class_list[doc_index])\n",
    "            \n",
    "        # the trainingMat is list-type, we need to convert it into np.array type\n",
    "        p0v, p1v, p_spam = NB.trainNB0(\n",
    "            np.array(trainingMat),\n",
    "            np.array(trainingClass)\n",
    "        )\n",
    "        # count the error classified result and print the error rate\n",
    "        errorCount = 0\n",
    "        for doc_index in testSetIndex:\n",
    "            # convert the doc in test set into feature vector\n",
    "            wordVec = NB.setOfWords2Vec(vocabList, doc_list[doc_index])\n",
    "            if NB.classifyNB(np.array(wordVec), p0v, p1v, p_spam) != class_list[doc_index]:\n",
    "                errorCount += 1\n",
    "        print ('the error rate is {}'.format(errorCount / len(testSetIndex)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is 0.0\n"
     ]
    }
   ],
   "source": [
    "spamClassifier = spamTestClassifier()\n",
    "spamClassifier.spam_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
